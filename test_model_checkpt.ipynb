{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: D:\\RL_Finance\\Image_to_Text\n"
     ]
    }
   ],
   "source": [
    "## Change the work dir:\n",
    "import os\n",
    "# Change to a new directory\n",
    "new_directory = \"D:/RL_Finance/Image_to_Text\"\n",
    "os.chdir(new_directory)\n",
    "\n",
    "\n",
    "\n",
    "# Verify the change\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "\n",
    "from PIL import Image\n",
    "from text_recognizer.data.create_save_argument_dataset import (load_processed_crops_and_labels, \n",
    "                                          ArgumentParagraphDataset, \n",
    "                                          DL_DATA_DIRNAME, \n",
    "                                          inverse_mapping,\n",
    "                                          mapping,\n",
    "                                          save_argument_data_as_tensors,\n",
    "                                          load_argument_data_as_tensors)\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from text_recognizer.stems.paragraph import ParagraphStem\n",
    "import text_recognizer.metadata.iam_paragraphs as metadata_iam_paragraphs\n",
    "from text_recognizer.models.resnet_transformer import ResnetTransformer\n",
    "from text_recognizer.data.base_data_module import BaseDataModule\n",
    "\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "input_dims=metadata_iam_paragraphs.DIMS\n",
    "output_dims=metadata_iam_paragraphs.OUTPUT_DIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/RL_Finance/data/downloaded/iam')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DL_DATA_DIRNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops, labels=load_processed_crops_and_labels(split=\"train\", data_dirname=DL_DATA_DIRNAME)\n",
    "dataset_len=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "argument_dataset = ArgumentParagraphDataset(\n",
    "    line_crops=crops,\n",
    "    line_labels=labels,\n",
    "    dataset_len=dataset_len,\n",
    "    inverse_mapping=inverse_mapping,\n",
    "    input_dims=input_dims,\n",
    "    output_dims=output_dims,\n",
    "    transform=ParagraphStem(augment=False),\n",
    ")\n",
    "\n",
    "# Generate training data\n",
    "argument_data = argument_dataset.generate_argument_paragraphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the argumented data\n",
    "save_argument_data_as_tensors(argument_data, DL_DATA_DIRNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RL_Finance\\MLops\\fslab\\lab06\\text_recognizer\\data\\create_save_argument_dataset.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  images = torch.load(save_dir / \"images.pt\")  # Load image tensors\n",
      "d:\\RL_Finance\\MLops\\fslab\\lab06\\text_recognizer\\data\\create_save_argument_dataset.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(save_dir / \"labels.pt\")  # Load label tensors\n"
     ]
    }
   ],
   "source": [
    "## Load the argumented dataset\n",
    "images, labels = load_argument_data_as_tensors(DL_DATA_DIRNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_DIM = 256\n",
    "TF_FC_DIM = 256\n",
    "TF_DROPOUT = 0.4\n",
    "TF_LAYERS = 4\n",
    "TF_NHEAD = 4\n",
    "\n",
    "RESNET_DIM = 512  # hard-coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    \"input_dims\": input_dims,  # (channels, height, width)\n",
    "    \"output_dims\": output_dims,  # Maximum output sequence length\n",
    "    \"mapping\": mapping,  # Example mapping for digits\n",
    "    \"inverse_mapping\": inverse_mapping,\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(tf_dim=TF_DIM, tf_fc_dim=TF_FC_DIM, tf_nhead=TF_NHEAD, tf_dropout=TF_DROPOUT, tf_layers=TF_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResnetTransformer(data_config, args).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path =r\"D:\\RL_Finance\\MLops\\fslab\\lab07\\text_recognizer\\artifacts\\paragraph-text-recognizer\\model.pt\"\n",
    "model_script = torch.jit.load(checkpoint_path, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dict = model_script.state_dict()\n",
    "## Make the keys names match\n",
    "new_state_dict = {k.replace(\"model.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data into Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.images, self.targets = load_argument_data_as_tensors(data_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "class CustomDataModule(BaseDataModule):  \n",
    "    def __init__(self, data_dir, batch_size, val_split=0.2):  \n",
    "        super().__init__()  \n",
    "        self.data_dir = data_dir  \n",
    "        self.batch_size = batch_size  \n",
    "        self.val_split = val_split  \n",
    "\n",
    "    def setup(self):  \n",
    "        dataset = CustomDataset(self.data_dir)  \n",
    "        val_size = int(len(dataset) * self.val_split)  \n",
    "        train_size = len(dataset) - val_size  \n",
    "\n",
    "        # Split into train and validation datasets\n",
    "        self.train_dataset, self.val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    def train_dataloader(self):  \n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)  \n",
    "\n",
    "    def val_dataloader(self):  \n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RL_Finance\\MLops\\fslab\\lab06\\text_recognizer\\data\\create_save_argument_dataset.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  images = torch.load(save_dir / \"images.pt\")  # Load image tensors\n",
      "d:\\RL_Finance\\MLops\\fslab\\lab06\\text_recognizer\\data\\create_save_argument_dataset.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(save_dir / \"labels.pt\")  # Load label tensors\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "S = data_config[\"output_dims\"][0]  # Output sequence length\n",
    "\n",
    "data_module = CustomDataModule(data_dir=DL_DATA_DIRNAME, batch_size=BATCH_SIZE)\n",
    "data_module.setup()  # Load data into train/val sets\n",
    "\n",
    "# Get DataLoader\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the model using data\n",
    "image, target = argument_data[1]\n",
    "\n",
    "image=image.unsqueeze(0)\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    image = image.to(device)\n",
    "    output = model(image)  # Encode image\n",
    "\n",
    "    \n",
    "    # Get the most likely label indices\n",
    "    ## predicted_labels = torch.argmax(logits, dim=1)  # (B, Sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S>permitted him to enjoy anything. 'The Pearl', he wrote,\n",
      "evening.\n",
      "cordid, and when she experiences it for the\n",
      "LORD SIDUEY WROTE TO DOUGUSS UNGHIDR\n",
      "the Director of Public Prosecutions I know petty\n",
      "chase of several mink coats which,\n",
      "a effective alleviation of his painful malady. None\n",
      "This phenamemon has nevertheless been\n",
      "almost unchanged in 1959 for couples with two or more\n",
      "in they May 1834.\n",
      "woodfiller in the usual way and paint the frame in\n",
      "from me almost instantly, but it had<E><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P>\n"
     ]
    }
   ],
   "source": [
    "decoded_text = ''.join(mapping[idx] for idx in output[0].tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S>permitted him to enjoy anything. 'The Pearl', he wrote,\n",
      "evening.\n",
      "sordid, and when she experiences it for the\n",
      "LORD SIDNEY WROTE TO DOUGLAS KINNAIRD\n",
      "the Director of Public Prosecutions I know pretty\n",
      "chase of several mink coats which,\n",
      "an effective alleviation of his painful malady. None\n",
      "again and again it is the visual qualities of\n",
      "This phenomenon has nevertheless been\n",
      "almost unchanged in 1959 for couples with two or more\n",
      "in # May 1834.\n",
      "woodfiller in the usual way and paint the frame in\n",
      "from me almost instantly, but it had<E><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P>\n"
     ]
    }
   ],
   "source": [
    "decoded_text = ''.join(mapping[idx] for idx in target.tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # -------- TRAINING --------\n",
    "        model.train()  # Set model to training mode\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            \n",
    "            # Teacher forcing\n",
    "\n",
    "            x = model.encode(x)  \n",
    "            logits = model.decode(x, y[:, :-1]).permute(1, 2, 0)\n",
    "\n",
    "            loss = criterion(logits, y[:, 1:])  \n",
    "            \n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] | Batch [{batch_idx+1}/{len(train_loader)}] | Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # -------- VALIDATION --------\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "                x = model.encode(x)\n",
    "                logits = model.decode(x, y[:, :-1]).permute(1, 2, 0)\n",
    "\n",
    "                loss = criterion(logits, y[:, 1:])    \n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        # Print Epoch Summary\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(\"Training complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4] | Batch [1/25] | Batch Loss: 1.0791\n",
      "Epoch [1/4] | Batch [2/25] | Batch Loss: 0.4445\n",
      "Epoch [1/4] | Batch [3/25] | Batch Loss: 0.4941\n",
      "Epoch [1/4] | Batch [4/25] | Batch Loss: 0.5003\n",
      "Epoch [1/4] | Batch [5/25] | Batch Loss: 0.2846\n",
      "Epoch [1/4] | Batch [6/25] | Batch Loss: 0.3435\n",
      "Epoch [1/4] | Batch [7/25] | Batch Loss: 0.2976\n",
      "Epoch [1/4] | Batch [8/25] | Batch Loss: 0.2524\n",
      "Epoch [1/4] | Batch [9/25] | Batch Loss: 0.3278\n",
      "Epoch [1/4] | Batch [10/25] | Batch Loss: 0.5325\n",
      "Epoch [1/4] | Batch [11/25] | Batch Loss: 0.2092\n",
      "Epoch [1/4] | Batch [12/25] | Batch Loss: 0.3254\n",
      "Epoch [1/4] | Batch [13/25] | Batch Loss: 0.3271\n",
      "Epoch [1/4] | Batch [14/25] | Batch Loss: 0.3206\n",
      "Epoch [1/4] | Batch [15/25] | Batch Loss: 0.2726\n",
      "Epoch [1/4] | Batch [16/25] | Batch Loss: 0.2964\n",
      "Epoch [1/4] | Batch [17/25] | Batch Loss: 0.2661\n",
      "Epoch [1/4] | Batch [18/25] | Batch Loss: 0.2278\n",
      "Epoch [1/4] | Batch [19/25] | Batch Loss: 0.1838\n",
      "Epoch [1/4] | Batch [20/25] | Batch Loss: 0.3044\n",
      "Epoch [1/4] | Batch [21/25] | Batch Loss: 0.2746\n",
      "Epoch [1/4] | Batch [22/25] | Batch Loss: 0.3108\n",
      "Epoch [1/4] | Batch [23/25] | Batch Loss: 0.2282\n",
      "Epoch [1/4] | Batch [24/25] | Batch Loss: 0.3183\n",
      "Epoch [1/4] | Batch [25/25] | Batch Loss: 0.3020\n",
      "Epoch [1/4] | Train Loss: 0.3489 | Val Loss: 0.1117\n",
      "Epoch [2/4] | Batch [1/25] | Batch Loss: 0.1779\n",
      "Epoch [2/4] | Batch [2/25] | Batch Loss: 0.1997\n",
      "Epoch [2/4] | Batch [3/25] | Batch Loss: 0.1527\n",
      "Epoch [2/4] | Batch [4/25] | Batch Loss: 0.1985\n",
      "Epoch [2/4] | Batch [5/25] | Batch Loss: 0.2400\n",
      "Epoch [2/4] | Batch [6/25] | Batch Loss: 0.2673\n",
      "Epoch [2/4] | Batch [7/25] | Batch Loss: 0.2160\n",
      "Epoch [2/4] | Batch [8/25] | Batch Loss: 0.2083\n",
      "Epoch [2/4] | Batch [9/25] | Batch Loss: 0.1178\n",
      "Epoch [2/4] | Batch [10/25] | Batch Loss: 0.2011\n",
      "Epoch [2/4] | Batch [11/25] | Batch Loss: 0.1834\n",
      "Epoch [2/4] | Batch [12/25] | Batch Loss: 0.1174\n",
      "Epoch [2/4] | Batch [13/25] | Batch Loss: 0.1535\n",
      "Epoch [2/4] | Batch [14/25] | Batch Loss: 0.1608\n",
      "Epoch [2/4] | Batch [15/25] | Batch Loss: 0.2009\n",
      "Epoch [2/4] | Batch [16/25] | Batch Loss: 0.2261\n",
      "Epoch [2/4] | Batch [17/25] | Batch Loss: 0.1367\n",
      "Epoch [2/4] | Batch [18/25] | Batch Loss: 0.1197\n",
      "Epoch [2/4] | Batch [19/25] | Batch Loss: 0.1460\n",
      "Epoch [2/4] | Batch [20/25] | Batch Loss: 0.1784\n",
      "Epoch [2/4] | Batch [21/25] | Batch Loss: 0.1868\n",
      "Epoch [2/4] | Batch [22/25] | Batch Loss: 0.1782\n",
      "Epoch [2/4] | Batch [23/25] | Batch Loss: 0.1503\n",
      "Epoch [2/4] | Batch [24/25] | Batch Loss: 0.2270\n",
      "Epoch [2/4] | Batch [25/25] | Batch Loss: 0.1637\n",
      "Epoch [2/4] | Train Loss: 0.1803 | Val Loss: 0.0917\n",
      "Epoch [3/4] | Batch [1/25] | Batch Loss: 0.1115\n",
      "Epoch [3/4] | Batch [2/25] | Batch Loss: 0.1991\n",
      "Epoch [3/4] | Batch [3/25] | Batch Loss: 0.2269\n",
      "Epoch [3/4] | Batch [4/25] | Batch Loss: 0.1300\n",
      "Epoch [3/4] | Batch [5/25] | Batch Loss: 0.0777\n",
      "Epoch [3/4] | Batch [6/25] | Batch Loss: 0.1267\n",
      "Epoch [3/4] | Batch [7/25] | Batch Loss: 0.1516\n",
      "Epoch [3/4] | Batch [8/25] | Batch Loss: 0.1154\n",
      "Epoch [3/4] | Batch [9/25] | Batch Loss: 0.1635\n",
      "Epoch [3/4] | Batch [10/25] | Batch Loss: 0.1019\n",
      "Epoch [3/4] | Batch [11/25] | Batch Loss: 0.1360\n",
      "Epoch [3/4] | Batch [12/25] | Batch Loss: 0.1096\n",
      "Epoch [3/4] | Batch [13/25] | Batch Loss: 0.1184\n",
      "Epoch [3/4] | Batch [14/25] | Batch Loss: 0.1216\n",
      "Epoch [3/4] | Batch [15/25] | Batch Loss: 0.1638\n",
      "Epoch [3/4] | Batch [16/25] | Batch Loss: 0.1941\n",
      "Epoch [3/4] | Batch [17/25] | Batch Loss: 0.1121\n",
      "Epoch [3/4] | Batch [18/25] | Batch Loss: 0.1341\n",
      "Epoch [3/4] | Batch [19/25] | Batch Loss: 0.0905\n",
      "Epoch [3/4] | Batch [20/25] | Batch Loss: 0.1701\n",
      "Epoch [3/4] | Batch [21/25] | Batch Loss: 0.0699\n",
      "Epoch [3/4] | Batch [22/25] | Batch Loss: 0.1616\n",
      "Epoch [3/4] | Batch [23/25] | Batch Loss: 0.1679\n",
      "Epoch [3/4] | Batch [24/25] | Batch Loss: 0.0854\n",
      "Epoch [3/4] | Batch [25/25] | Batch Loss: 0.1590\n",
      "Epoch [3/4] | Train Loss: 0.1359 | Val Loss: 0.0818\n",
      "Epoch [4/4] | Batch [1/25] | Batch Loss: 0.1004\n",
      "Epoch [4/4] | Batch [2/25] | Batch Loss: 0.1584\n",
      "Epoch [4/4] | Batch [3/25] | Batch Loss: 0.0963\n",
      "Epoch [4/4] | Batch [4/25] | Batch Loss: 0.1780\n",
      "Epoch [4/4] | Batch [5/25] | Batch Loss: 0.0984\n",
      "Epoch [4/4] | Batch [6/25] | Batch Loss: 0.1705\n",
      "Epoch [4/4] | Batch [7/25] | Batch Loss: 0.1442\n",
      "Epoch [4/4] | Batch [8/25] | Batch Loss: 0.1216\n",
      "Epoch [4/4] | Batch [9/25] | Batch Loss: 0.1120\n",
      "Epoch [4/4] | Batch [10/25] | Batch Loss: 0.1066\n",
      "Epoch [4/4] | Batch [11/25] | Batch Loss: 0.0832\n",
      "Epoch [4/4] | Batch [12/25] | Batch Loss: 0.1293\n",
      "Epoch [4/4] | Batch [13/25] | Batch Loss: 0.0823\n",
      "Epoch [4/4] | Batch [14/25] | Batch Loss: 0.0782\n",
      "Epoch [4/4] | Batch [15/25] | Batch Loss: 0.0852\n",
      "Epoch [4/4] | Batch [16/25] | Batch Loss: 0.1076\n",
      "Epoch [4/4] | Batch [17/25] | Batch Loss: 0.0776\n",
      "Epoch [4/4] | Batch [18/25] | Batch Loss: 0.0952\n",
      "Epoch [4/4] | Batch [19/25] | Batch Loss: 0.1759\n",
      "Epoch [4/4] | Batch [20/25] | Batch Loss: 0.0967\n",
      "Epoch [4/4] | Batch [21/25] | Batch Loss: 0.0985\n",
      "Epoch [4/4] | Batch [22/25] | Batch Loss: 0.0758\n",
      "Epoch [4/4] | Batch [23/25] | Batch Loss: 0.1221\n",
      "Epoch [4/4] | Batch [24/25] | Batch Loss: 0.0950\n",
      "Epoch [4/4] | Batch [25/25] | Batch Loss: 0.0861\n",
      "Epoch [4/4] | Train Loss: 0.1110 | Val Loss: 0.0800\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a simple test on picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = \"a01-077.png\"\n",
    "# Open the image\n",
    "image = Image.open(example_input)\n",
    "transform=ParagraphStem(augment=False)\n",
    "image_tensor = transform(image)\n",
    "\n",
    "image_tensor=image_tensor.unsqueeze(0)\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    output = model(image_tensor)  # Encode image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S>And, since this is election gear in West\n",
      "Germany, Dr. Adenauer is in a tough\n",
      "spot. Joyce Egginton cables: President\n",
      "Kennedy at his Washington Press con-\n",
      "ference admitted he did not know\n",
      "Whether America was lagging behind\n",
      "Russia in missile power. He said he\n",
      "Was waiting for his senior military\n",
      "aides to come up with the answer on\n",
      "February 20.<E><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P>\n"
     ]
    }
   ],
   "source": [
    "decoded_text = ''.join(mapping[idx] for idx in output[0].tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LORA fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "target_module_1 = [\n",
    "    \"self_attn.in_proj_weight\",\n",
    "    \"self_attn.out_proj\",\n",
    "    \"multihead_attn.in_proj_weight\",\n",
    "    \"multihead_attn.out_proj\",\n",
    "    \"linear1\",\n",
    "    \"linear2\",\n",
    "]\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,   # Rank of decomposition\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_module_1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 65,536 || all params: 14,054,292 || trainable%: 0.4663\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Check trainable parameters\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=data_config[\"inverse_mapping\"][\"<P>\"])  # Ignore padding token\n",
    "optimizer = optim.AdamW(lora_model.parameters(), lr=0.0001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6] | Batch [1/25] | Batch Loss: 0.7589\n",
      "Epoch [1/6] | Batch [2/25] | Batch Loss: 0.6345\n",
      "Epoch [1/6] | Batch [3/25] | Batch Loss: 0.8879\n",
      "Epoch [1/6] | Batch [4/25] | Batch Loss: 0.8949\n",
      "Epoch [1/6] | Batch [5/25] | Batch Loss: 1.8075\n",
      "Epoch [1/6] | Batch [6/25] | Batch Loss: 1.1619\n",
      "Epoch [1/6] | Batch [7/25] | Batch Loss: 0.9878\n",
      "Epoch [1/6] | Batch [8/25] | Batch Loss: 0.6346\n",
      "Epoch [1/6] | Batch [9/25] | Batch Loss: 0.9811\n",
      "Epoch [1/6] | Batch [10/25] | Batch Loss: 1.3444\n",
      "Epoch [1/6] | Batch [11/25] | Batch Loss: 0.9411\n",
      "Epoch [1/6] | Batch [12/25] | Batch Loss: 0.6562\n",
      "Epoch [1/6] | Batch [13/25] | Batch Loss: 0.9765\n",
      "Epoch [1/6] | Batch [14/25] | Batch Loss: 0.6465\n",
      "Epoch [1/6] | Batch [15/25] | Batch Loss: 0.7510\n",
      "Epoch [1/6] | Batch [16/25] | Batch Loss: 1.0983\n",
      "Epoch [1/6] | Batch [17/25] | Batch Loss: 0.8488\n",
      "Epoch [1/6] | Batch [18/25] | Batch Loss: 0.5181\n",
      "Epoch [1/6] | Batch [19/25] | Batch Loss: 0.7783\n",
      "Epoch [1/6] | Batch [20/25] | Batch Loss: 0.9842\n",
      "Epoch [1/6] | Batch [21/25] | Batch Loss: 0.5945\n",
      "Epoch [1/6] | Batch [22/25] | Batch Loss: 0.7007\n",
      "Epoch [1/6] | Batch [23/25] | Batch Loss: 0.9534\n",
      "Epoch [1/6] | Batch [24/25] | Batch Loss: 1.0488\n",
      "Epoch [1/6] | Batch [25/25] | Batch Loss: 0.6834\n",
      "Epoch [1/6] | Train Loss: 0.8909 | Val Loss: 0.4730\n",
      "Epoch [2/6] | Batch [1/25] | Batch Loss: 1.8324\n",
      "Epoch [2/6] | Batch [2/25] | Batch Loss: 1.1219\n",
      "Epoch [2/6] | Batch [3/25] | Batch Loss: 0.9124\n",
      "Epoch [2/6] | Batch [4/25] | Batch Loss: 0.6915\n",
      "Epoch [2/6] | Batch [5/25] | Batch Loss: 0.7294\n",
      "Epoch [2/6] | Batch [6/25] | Batch Loss: 0.4727\n",
      "Epoch [2/6] | Batch [7/25] | Batch Loss: 0.9344\n",
      "Epoch [2/6] | Batch [8/25] | Batch Loss: 0.4414\n",
      "Epoch [2/6] | Batch [9/25] | Batch Loss: 0.8452\n",
      "Epoch [2/6] | Batch [10/25] | Batch Loss: 0.8022\n",
      "Epoch [2/6] | Batch [11/25] | Batch Loss: 0.7946\n",
      "Epoch [2/6] | Batch [12/25] | Batch Loss: 0.5390\n",
      "Epoch [2/6] | Batch [13/25] | Batch Loss: 0.7914\n",
      "Epoch [2/6] | Batch [14/25] | Batch Loss: 0.8089\n",
      "Epoch [2/6] | Batch [15/25] | Batch Loss: 0.7068\n",
      "Epoch [2/6] | Batch [16/25] | Batch Loss: 0.7583\n",
      "Epoch [2/6] | Batch [17/25] | Batch Loss: 1.0615\n",
      "Epoch [2/6] | Batch [18/25] | Batch Loss: 0.7006\n",
      "Epoch [2/6] | Batch [19/25] | Batch Loss: 0.8372\n",
      "Epoch [2/6] | Batch [20/25] | Batch Loss: 0.6735\n",
      "Epoch [2/6] | Batch [21/25] | Batch Loss: 1.2455\n",
      "Epoch [2/6] | Batch [22/25] | Batch Loss: 0.6451\n",
      "Epoch [2/6] | Batch [23/25] | Batch Loss: 1.2544\n",
      "Epoch [2/6] | Batch [24/25] | Batch Loss: 0.8958\n",
      "Epoch [2/6] | Batch [25/25] | Batch Loss: 0.5652\n",
      "Epoch [2/6] | Train Loss: 0.8425 | Val Loss: 0.4849\n",
      "Epoch [3/6] | Batch [1/25] | Batch Loss: 0.5969\n",
      "Epoch [3/6] | Batch [2/25] | Batch Loss: 0.8482\n",
      "Epoch [3/6] | Batch [3/25] | Batch Loss: 0.5941\n",
      "Epoch [3/6] | Batch [4/25] | Batch Loss: 0.4535\n",
      "Epoch [3/6] | Batch [5/25] | Batch Loss: 0.7316\n",
      "Epoch [3/6] | Batch [6/25] | Batch Loss: 0.6498\n",
      "Epoch [3/6] | Batch [7/25] | Batch Loss: 0.6793\n",
      "Epoch [3/6] | Batch [8/25] | Batch Loss: 0.9263\n",
      "Epoch [3/6] | Batch [9/25] | Batch Loss: 1.1221\n",
      "Epoch [3/6] | Batch [10/25] | Batch Loss: 1.0655\n",
      "Epoch [3/6] | Batch [11/25] | Batch Loss: 0.6526\n",
      "Epoch [3/6] | Batch [12/25] | Batch Loss: 0.7920\n",
      "Epoch [3/6] | Batch [13/25] | Batch Loss: 0.6794\n",
      "Epoch [3/6] | Batch [14/25] | Batch Loss: 0.8563\n",
      "Epoch [3/6] | Batch [15/25] | Batch Loss: 0.8008\n",
      "Epoch [3/6] | Batch [16/25] | Batch Loss: 0.8440\n",
      "Epoch [3/6] | Batch [17/25] | Batch Loss: 0.8336\n",
      "Epoch [3/6] | Batch [18/25] | Batch Loss: 0.6749\n",
      "Epoch [3/6] | Batch [19/25] | Batch Loss: 0.8984\n",
      "Epoch [3/6] | Batch [20/25] | Batch Loss: 0.5018\n",
      "Epoch [3/6] | Batch [21/25] | Batch Loss: 0.6576\n",
      "Epoch [3/6] | Batch [22/25] | Batch Loss: 0.7692\n",
      "Epoch [3/6] | Batch [23/25] | Batch Loss: 0.4419\n",
      "Epoch [3/6] | Batch [24/25] | Batch Loss: 0.4716\n",
      "Epoch [3/6] | Batch [25/25] | Batch Loss: 0.6210\n",
      "Epoch [3/6] | Train Loss: 0.7265 | Val Loss: 0.3900\n",
      "Epoch [4/6] | Batch [1/25] | Batch Loss: 0.5887\n",
      "Epoch [4/6] | Batch [2/25] | Batch Loss: 0.9503\n",
      "Epoch [4/6] | Batch [3/25] | Batch Loss: 0.4779\n",
      "Epoch [4/6] | Batch [4/25] | Batch Loss: 0.8435\n",
      "Epoch [4/6] | Batch [5/25] | Batch Loss: 0.7057\n",
      "Epoch [4/6] | Batch [6/25] | Batch Loss: 0.5225\n",
      "Epoch [4/6] | Batch [7/25] | Batch Loss: 1.0366\n",
      "Epoch [4/6] | Batch [8/25] | Batch Loss: 0.5655\n",
      "Epoch [4/6] | Batch [9/25] | Batch Loss: 0.5672\n",
      "Epoch [4/6] | Batch [10/25] | Batch Loss: 0.8207\n",
      "Epoch [4/6] | Batch [11/25] | Batch Loss: 0.6499\n",
      "Epoch [4/6] | Batch [12/25] | Batch Loss: 0.7313\n",
      "Epoch [4/6] | Batch [13/25] | Batch Loss: 0.4234\n",
      "Epoch [4/6] | Batch [14/25] | Batch Loss: 0.6051\n",
      "Epoch [4/6] | Batch [15/25] | Batch Loss: 0.5660\n",
      "Epoch [4/6] | Batch [16/25] | Batch Loss: 0.4074\n",
      "Epoch [4/6] | Batch [17/25] | Batch Loss: 0.3443\n",
      "Epoch [4/6] | Batch [18/25] | Batch Loss: 0.9866\n",
      "Epoch [4/6] | Batch [19/25] | Batch Loss: 0.6784\n",
      "Epoch [4/6] | Batch [20/25] | Batch Loss: 0.7978\n",
      "Epoch [4/6] | Batch [21/25] | Batch Loss: 0.6300\n",
      "Epoch [4/6] | Batch [22/25] | Batch Loss: 0.3798\n",
      "Epoch [4/6] | Batch [23/25] | Batch Loss: 0.6425\n",
      "Epoch [4/6] | Batch [24/25] | Batch Loss: 0.5620\n",
      "Epoch [4/6] | Batch [25/25] | Batch Loss: 0.4868\n",
      "Epoch [4/6] | Train Loss: 0.6388 | Val Loss: 0.3297\n",
      "Epoch [5/6] | Batch [1/25] | Batch Loss: 0.4763\n",
      "Epoch [5/6] | Batch [2/25] | Batch Loss: 0.4986\n",
      "Epoch [5/6] | Batch [3/25] | Batch Loss: 0.7304\n",
      "Epoch [5/6] | Batch [4/25] | Batch Loss: 0.5303\n",
      "Epoch [5/6] | Batch [5/25] | Batch Loss: 0.5565\n",
      "Epoch [5/6] | Batch [6/25] | Batch Loss: 0.5176\n",
      "Epoch [5/6] | Batch [7/25] | Batch Loss: 0.3049\n",
      "Epoch [5/6] | Batch [8/25] | Batch Loss: 0.6390\n",
      "Epoch [5/6] | Batch [9/25] | Batch Loss: 0.5315\n",
      "Epoch [5/6] | Batch [10/25] | Batch Loss: 0.6664\n",
      "Epoch [5/6] | Batch [11/25] | Batch Loss: 0.5189\n",
      "Epoch [5/6] | Batch [12/25] | Batch Loss: 0.6433\n",
      "Epoch [5/6] | Batch [13/25] | Batch Loss: 0.8779\n",
      "Epoch [5/6] | Batch [14/25] | Batch Loss: 0.4292\n",
      "Epoch [5/6] | Batch [15/25] | Batch Loss: 0.4419\n",
      "Epoch [5/6] | Batch [16/25] | Batch Loss: 0.5426\n",
      "Epoch [5/6] | Batch [17/25] | Batch Loss: 0.5971\n",
      "Epoch [5/6] | Batch [18/25] | Batch Loss: 0.9696\n",
      "Epoch [5/6] | Batch [19/25] | Batch Loss: 0.7274\n",
      "Epoch [5/6] | Batch [20/25] | Batch Loss: 0.5130\n",
      "Epoch [5/6] | Batch [21/25] | Batch Loss: 0.7295\n",
      "Epoch [5/6] | Batch [22/25] | Batch Loss: 0.9077\n",
      "Epoch [5/6] | Batch [23/25] | Batch Loss: 0.2997\n",
      "Epoch [5/6] | Batch [24/25] | Batch Loss: 0.7328\n",
      "Epoch [5/6] | Batch [25/25] | Batch Loss: 0.5941\n",
      "Epoch [5/6] | Train Loss: 0.5990 | Val Loss: 0.3191\n",
      "Epoch [6/6] | Batch [1/25] | Batch Loss: 0.4573\n",
      "Epoch [6/6] | Batch [2/25] | Batch Loss: 0.3837\n",
      "Epoch [6/6] | Batch [3/25] | Batch Loss: 0.5968\n",
      "Epoch [6/6] | Batch [4/25] | Batch Loss: 0.3816\n",
      "Epoch [6/6] | Batch [5/25] | Batch Loss: 0.9744\n",
      "Epoch [6/6] | Batch [6/25] | Batch Loss: 0.5796\n",
      "Epoch [6/6] | Batch [7/25] | Batch Loss: 0.5734\n",
      "Epoch [6/6] | Batch [8/25] | Batch Loss: 0.4421\n",
      "Epoch [6/6] | Batch [9/25] | Batch Loss: 0.6482\n",
      "Epoch [6/6] | Batch [10/25] | Batch Loss: 0.4747\n",
      "Epoch [6/6] | Batch [11/25] | Batch Loss: 0.7878\n",
      "Epoch [6/6] | Batch [12/25] | Batch Loss: 0.3944\n",
      "Epoch [6/6] | Batch [13/25] | Batch Loss: 0.5801\n",
      "Epoch [6/6] | Batch [14/25] | Batch Loss: 0.4683\n",
      "Epoch [6/6] | Batch [15/25] | Batch Loss: 0.5137\n",
      "Epoch [6/6] | Batch [16/25] | Batch Loss: 0.6151\n",
      "Epoch [6/6] | Batch [17/25] | Batch Loss: 0.5846\n",
      "Epoch [6/6] | Batch [18/25] | Batch Loss: 0.9163\n",
      "Epoch [6/6] | Batch [19/25] | Batch Loss: 0.4691\n",
      "Epoch [6/6] | Batch [20/25] | Batch Loss: 1.1722\n",
      "Epoch [6/6] | Batch [21/25] | Batch Loss: 0.5679\n",
      "Epoch [6/6] | Batch [22/25] | Batch Loss: 0.6255\n",
      "Epoch [6/6] | Batch [23/25] | Batch Loss: 0.3859\n",
      "Epoch [6/6] | Batch [24/25] | Batch Loss: 0.5097\n",
      "Epoch [6/6] | Batch [25/25] | Batch Loss: 0.3645\n",
      "Epoch [6/6] | Train Loss: 0.5787 | Val Loss: 0.2830\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train_model(lora_model, train_loader, val_loader, optimizer, criterion, device, num_epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the single picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = \"a01-077.png\"\n",
    "# Open the image\n",
    "image = Image.open(example_input)\n",
    "transform=ParagraphStem(augment=False)\n",
    "image_tensor = transform(image)\n",
    "\n",
    "image_tensor=image_tensor.unsqueeze(0)\n",
    "\n",
    "lora_model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    output = lora_model(image_tensor)  # Encode image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S>And, since this is election year in West\n",
      "Germany, Dr. Adenauer is in a tough\n",
      "spot. Joyce Egginton cables: President\n",
      "ference admitted he did not know\n",
      "ference admitted he did not know\n",
      "Russia in missile power. He said he\n",
      "Russia in missile power. He said he\n",
      "was waiting for his senior military\n",
      "aides to come Up with the answer on\n",
      "February 2.<E><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P>\n"
     ]
    }
   ],
   "source": [
    "decoded_text = ''.join(mapping[idx] for idx in output[0].tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lighning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: D:\\RL_Finance\\MLops\\fslab\\lab06\n"
     ]
    }
   ],
   "source": [
    "## Change the work dir:\n",
    "import os\n",
    "# Change to a new directory\n",
    "new_directory = \"D:/RL_Finance/MLops/fslab/lab06\"\n",
    "os.chdir(new_directory)\n",
    "\n",
    "\n",
    "\n",
    "# Verify the change\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\xiang\\_netrc\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxiangyexu\u001b[0m (\u001b[33mxiangyexu-university-of-waterloo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\RL_Finance\\MLops\\fslab\\lab06\\wandb\\run-20250406_130058-935udiyr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xiangyexu-university-of-waterloo/image_to_text/runs/935udiyr' target=\"_blank\">empathic-kirk-3</a></strong> to <a href='https://wandb.ai/xiangyexu-university-of-waterloo/image_to_text' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xiangyexu-university-of-waterloo/image_to_text' target=\"_blank\">https://wandb.ai/xiangyexu-university-of-waterloo/image_to_text</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xiangyexu-university-of-waterloo/image_to_text/runs/935udiyr' target=\"_blank\">https://wandb.ai/xiangyexu-university-of-waterloo/image_to_text/runs/935udiyr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/xiangyexu-university-of-waterloo/image_to_text/runs/935udiyr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1f49da49db0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "!wandb login 9cea57a44f7df81e3434dc972ef754421915e27e\n",
    "\n",
    "\n",
    "wandb.init(project=\"image_to_text\",resume=\"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:62: FutureWarning: Importing `CharErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `CharErrorRate` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\lightning_fabric\\connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "INFO: Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO: Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "D:\\RL_Finance\\MLops\\fslab\\lab06\\text_recognizer\\data\\create_save_argument_dataset.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  images = torch.load(save_dir / \"images.pt\")  # Load image tensors\n",
      "D:\\RL_Finance\\MLops\\fslab\\lab06\\text_recognizer\\data\\create_save_argument_dataset.py:117: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(save_dir / \"labels.pt\")  # Load label tensors\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type               | Params | Mode \n",
      "----------------------------------------------------------------\n",
      "0 | model            | PeftModel          | 14.1 M | train\n",
      "1 | model.base_model | LoraModel          | 14.1 M | train\n",
      "2 | train_acc        | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc          | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc         | MulticlassAccuracy | 0      | train\n",
      "5 | val_cer          | CharacterErrorRate | 0      | train\n",
      "6 | test_cer         | CharacterErrorRate | 0      | train\n",
      "7 | loss_fn          | CrossEntropyLoss   | 0      | train\n",
      "----------------------------------------------------------------\n",
      "65.5 K    Trainable params\n",
      "14.0 M    Non-trainable params\n",
      "14.1 M    Total params\n",
      "56.217    Total estimated model params size (MB)\n",
      "299       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model State Dict Disk Size: 434.55 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>optimizer/lr-Adam</td><td>▁▁▁▁▁</td></tr><tr><td>size/mb_disk</td><td>▁</td></tr><tr><td>size/nparams</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▃▄▄▅▅▇▇█</td></tr><tr><td>validation/cer</td><td>▁▂▆█▅</td></tr><tr><td>validation/loss</td><td>█▃▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>optimizer/lr-Adam</td><td>0.001</td></tr><tr><td>size/mb_disk</td><td>434.54845</td></tr><tr><td>size/nparams</td><td>14054292</td></tr><tr><td>trainer/global_step</td><td>24</td></tr><tr><td>validation/cer</td><td>0.19025</td></tr><tr><td>validation/loss</td><td>0.2271</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">empathic-kirk-3</strong> at: <a href='https://wandb.ai/xiangyexu-university-of-waterloo/image_to_text/runs/935udiyr' target=\"_blank\">https://wandb.ai/xiangyexu-university-of-waterloo/image_to_text/runs/935udiyr</a><br> View project at: <a href='https://wandb.ai/xiangyexu-university-of-waterloo/image_to_text' target=\"_blank\">https://wandb.ai/xiangyexu-university-of-waterloo/image_to_text</a><br>Synced 5 W&B file(s), 6 media file(s), 38 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250406_130058-935udiyr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.2 s\n",
      "Wall time: 6min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run run_experiment.py --batch_size 16 --max_epochs 5 --wandb\n",
    "\n",
    "last_expt = wandb.run\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
