{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: D:\\RL_Finance\\Image_to_Text\n"
     ]
    }
   ],
   "source": [
    "## Change the work dir:\n",
    "import os\n",
    "# Change to a new directory\n",
    "new_directory = \"D:/RL_Finance/Image_to_Text\"\n",
    "os.chdir(new_directory)\n",
    "\n",
    "\n",
    "\n",
    "# Verify the change\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "\n",
    "from PIL import Image\n",
    "from text_recognizer.data.create_save_argument_dataset import (load_processed_crops_and_labels, \n",
    "                                          ArgumentParagraphDataset, \n",
    "                                          DL_DATA_DIRNAME, \n",
    "                                          inverse_mapping,\n",
    "                                          mapping,\n",
    "                                          save_argument_data_as_tensors,\n",
    "                                          load_argument_data_as_tensors,\n",
    "                                          extract_images_and_labels)\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from text_recognizer.stems.paragraph import ParagraphStem\n",
    "import text_recognizer.metadata.iam_paragraphs as metadata_iam_paragraphs\n",
    "from text_recognizer.models.resnet_transformer import ResnetTransformer\n",
    "from text_recognizer.data.base_data_module import BaseDataModule\n",
    "\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_dims=metadata_iam_paragraphs.DIMS\n",
    "output_dims=metadata_iam_paragraphs.OUTPUT_DIMS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_DIM = 256\n",
    "TF_FC_DIM = 256\n",
    "TF_DROPOUT = 0.4\n",
    "TF_LAYERS = 4\n",
    "TF_NHEAD = 4\n",
    "\n",
    "RESNET_DIM = 512  # hard-coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    \"input_dims\": input_dims,  # (channels, height, width)\n",
    "    \"output_dims\": output_dims,  # Maximum output sequence length\n",
    "    \"mapping\": mapping,  # Example mapping for digits\n",
    "    \"inverse_mapping\": inverse_mapping,\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(tf_dim=TF_DIM, tf_fc_dim=TF_FC_DIM, tf_nhead=TF_NHEAD, tf_dropout=TF_DROPOUT, tf_layers=TF_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResnetTransformer(data_config, args).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path =r\"D:\\RL_Finance\\MLops\\fslab\\lab07\\text_recognizer\\artifacts\\paragraph-text-recognizer\\model.pt\"\n",
    "model_script = torch.jit.load(checkpoint_path, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dict = model_script.state_dict()\n",
    "\n",
    "\n",
    "## Make the keys names match\n",
    "new_state_dict = {k.replace(\"model.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data into Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, dataset_len):\n",
    "        crops, labels=load_processed_crops_and_labels(split=\"train\", data_dirname=data_dir)\n",
    "        # Create dataset\n",
    "        argument_dataset = ArgumentParagraphDataset(\n",
    "            line_crops=crops,\n",
    "            line_labels=labels,\n",
    "            dataset_len=dataset_len,\n",
    "            inverse_mapping=inverse_mapping,\n",
    "            input_dims=input_dims,\n",
    "            output_dims=output_dims,\n",
    "            transform=ParagraphStem(augment=False),\n",
    "        )\n",
    "\n",
    "        # Generate training data\n",
    "        argument_data = argument_dataset.generate_argument_paragraphs()\n",
    "        self.images, self.targets = extract_images_and_labels(argument_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "class CustomDataModule(BaseDataModule):  \n",
    "    def __init__(self, data_dir, dataset_len, batch_size, val_split=0.2):  \n",
    "        super().__init__()  \n",
    "        self.data_dir = data_dir  \n",
    "        self.dataset_len=dataset_len\n",
    "        self.batch_size = batch_size  \n",
    "        self.val_split = val_split  \n",
    "\n",
    "    def setup(self):  \n",
    "        dataset = CustomDataset(self.data_dir, self.dataset_len)  \n",
    "        val_size = int(len(dataset) * self.val_split)  \n",
    "        train_size = len(dataset) - val_size  \n",
    "\n",
    "        # Split into train and validation datasets\n",
    "        self.train_dataset, self.val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    def train_dataloader(self):  \n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)  \n",
    "\n",
    "    def val_dataloader(self):  \n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "S = data_config[\"output_dims\"][0]  # Output sequence length\n",
    "\n",
    "data_module = CustomDataModule(data_dir=DL_DATA_DIRNAME, dataset_len=1000, batch_size=BATCH_SIZE)\n",
    "data_module.setup()  # Load data into train/val sets\n",
    "\n",
    "# Get DataLoader\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the model using data\n",
    "image, target = argument_data[1]\n",
    "\n",
    "image=image.unsqueeze(0)\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    image = image.to(device)\n",
    "    output = model(image)  # Encode image\n",
    "\n",
    "    \n",
    "    # Get the most likely label indices\n",
    "    ## predicted_labels = torch.argmax(logits, dim=1)  # (B, Sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S>permitted him to enjoy anything. 'The Pearl', he wrote,\n",
      "evening.\n",
      "cordid, and when she experiences it for the\n",
      "LORD SIDUEY WROTE TO DOUGUSS UNGHIDR\n",
      "the Director of Public Prosecutions I know petty\n",
      "chase of several mink coats which,\n",
      "a effective alleviation of his painful malady. None\n",
      "This phenamemon has nevertheless been\n",
      "almost unchanged in 1959 for couples with two or more\n",
      "in they May 1834.\n",
      "woodfiller in the usual way and paint the frame in\n",
      "from me almost instantly, but it had<E><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P>\n"
     ]
    }
   ],
   "source": [
    "decoded_text = ''.join(mapping[idx] for idx in output[0].tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S>permitted him to enjoy anything. 'The Pearl', he wrote,\n",
      "evening.\n",
      "sordid, and when she experiences it for the\n",
      "LORD SIDNEY WROTE TO DOUGLAS KINNAIRD\n",
      "the Director of Public Prosecutions I know pretty\n",
      "chase of several mink coats which,\n",
      "an effective alleviation of his painful malady. None\n",
      "again and again it is the visual qualities of\n",
      "This phenomenon has nevertheless been\n",
      "almost unchanged in 1959 for couples with two or more\n",
      "in # May 1834.\n",
      "woodfiller in the usual way and paint the frame in\n",
      "from me almost instantly, but it had<E><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P>\n"
     ]
    }
   ],
   "source": [
    "decoded_text = ''.join(mapping[idx] for idx in target.tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # -------- TRAINING --------\n",
    "        model.train()  # Set model to training mode\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            \n",
    "            # Teacher forcing\n",
    "\n",
    "            x = model.encode(x)  \n",
    "            logits = model.decode(x, y[:, :-1]).permute(1, 2, 0)\n",
    "\n",
    "            loss = criterion(logits, y[:, 1:])  \n",
    "            \n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] | Batch [{batch_idx+1}/{len(train_loader)}] | Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # -------- VALIDATION --------\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "                x = model.encode(x)\n",
    "                logits = model.decode(x, y[:, :-1]).permute(1, 2, 0)\n",
    "\n",
    "                loss = criterion(logits, y[:, 1:])    \n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        # Print Epoch Summary\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(\"Training complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4] | Batch [1/25] | Batch Loss: 1.0791\n",
      "Epoch [1/4] | Batch [2/25] | Batch Loss: 0.4445\n",
      "Epoch [1/4] | Batch [3/25] | Batch Loss: 0.4941\n",
      "Epoch [1/4] | Batch [4/25] | Batch Loss: 0.5003\n",
      "Epoch [1/4] | Batch [5/25] | Batch Loss: 0.2846\n",
      "Epoch [1/4] | Batch [6/25] | Batch Loss: 0.3435\n",
      "Epoch [1/4] | Batch [7/25] | Batch Loss: 0.2976\n",
      "Epoch [1/4] | Batch [8/25] | Batch Loss: 0.2524\n",
      "Epoch [1/4] | Batch [9/25] | Batch Loss: 0.3278\n",
      "Epoch [1/4] | Batch [10/25] | Batch Loss: 0.5325\n",
      "Epoch [1/4] | Batch [11/25] | Batch Loss: 0.2092\n",
      "Epoch [1/4] | Batch [12/25] | Batch Loss: 0.3254\n",
      "Epoch [1/4] | Batch [13/25] | Batch Loss: 0.3271\n",
      "Epoch [1/4] | Batch [14/25] | Batch Loss: 0.3206\n",
      "Epoch [1/4] | Batch [15/25] | Batch Loss: 0.2726\n",
      "Epoch [1/4] | Batch [16/25] | Batch Loss: 0.2964\n",
      "Epoch [1/4] | Batch [17/25] | Batch Loss: 0.2661\n",
      "Epoch [1/4] | Batch [18/25] | Batch Loss: 0.2278\n",
      "Epoch [1/4] | Batch [19/25] | Batch Loss: 0.1838\n",
      "Epoch [1/4] | Batch [20/25] | Batch Loss: 0.3044\n",
      "Epoch [1/4] | Batch [21/25] | Batch Loss: 0.2746\n",
      "Epoch [1/4] | Batch [22/25] | Batch Loss: 0.3108\n",
      "Epoch [1/4] | Batch [23/25] | Batch Loss: 0.2282\n",
      "Epoch [1/4] | Batch [24/25] | Batch Loss: 0.3183\n",
      "Epoch [1/4] | Batch [25/25] | Batch Loss: 0.3020\n",
      "Epoch [1/4] | Train Loss: 0.3489 | Val Loss: 0.1117\n",
      "Epoch [2/4] | Batch [1/25] | Batch Loss: 0.1779\n",
      "Epoch [2/4] | Batch [2/25] | Batch Loss: 0.1997\n",
      "Epoch [2/4] | Batch [3/25] | Batch Loss: 0.1527\n",
      "Epoch [2/4] | Batch [4/25] | Batch Loss: 0.1985\n",
      "Epoch [2/4] | Batch [5/25] | Batch Loss: 0.2400\n",
      "Epoch [2/4] | Batch [6/25] | Batch Loss: 0.2673\n",
      "Epoch [2/4] | Batch [7/25] | Batch Loss: 0.2160\n",
      "Epoch [2/4] | Batch [8/25] | Batch Loss: 0.2083\n",
      "Epoch [2/4] | Batch [9/25] | Batch Loss: 0.1178\n",
      "Epoch [2/4] | Batch [10/25] | Batch Loss: 0.2011\n",
      "Epoch [2/4] | Batch [11/25] | Batch Loss: 0.1834\n",
      "Epoch [2/4] | Batch [12/25] | Batch Loss: 0.1174\n",
      "Epoch [2/4] | Batch [13/25] | Batch Loss: 0.1535\n",
      "Epoch [2/4] | Batch [14/25] | Batch Loss: 0.1608\n",
      "Epoch [2/4] | Batch [15/25] | Batch Loss: 0.2009\n",
      "Epoch [2/4] | Batch [16/25] | Batch Loss: 0.2261\n",
      "Epoch [2/4] | Batch [17/25] | Batch Loss: 0.1367\n",
      "Epoch [2/4] | Batch [18/25] | Batch Loss: 0.1197\n",
      "Epoch [2/4] | Batch [19/25] | Batch Loss: 0.1460\n",
      "Epoch [2/4] | Batch [20/25] | Batch Loss: 0.1784\n",
      "Epoch [2/4] | Batch [21/25] | Batch Loss: 0.1868\n",
      "Epoch [2/4] | Batch [22/25] | Batch Loss: 0.1782\n",
      "Epoch [2/4] | Batch [23/25] | Batch Loss: 0.1503\n",
      "Epoch [2/4] | Batch [24/25] | Batch Loss: 0.2270\n",
      "Epoch [2/4] | Batch [25/25] | Batch Loss: 0.1637\n",
      "Epoch [2/4] | Train Loss: 0.1803 | Val Loss: 0.0917\n",
      "Epoch [3/4] | Batch [1/25] | Batch Loss: 0.1115\n",
      "Epoch [3/4] | Batch [2/25] | Batch Loss: 0.1991\n",
      "Epoch [3/4] | Batch [3/25] | Batch Loss: 0.2269\n",
      "Epoch [3/4] | Batch [4/25] | Batch Loss: 0.1300\n",
      "Epoch [3/4] | Batch [5/25] | Batch Loss: 0.0777\n",
      "Epoch [3/4] | Batch [6/25] | Batch Loss: 0.1267\n",
      "Epoch [3/4] | Batch [7/25] | Batch Loss: 0.1516\n",
      "Epoch [3/4] | Batch [8/25] | Batch Loss: 0.1154\n",
      "Epoch [3/4] | Batch [9/25] | Batch Loss: 0.1635\n",
      "Epoch [3/4] | Batch [10/25] | Batch Loss: 0.1019\n",
      "Epoch [3/4] | Batch [11/25] | Batch Loss: 0.1360\n",
      "Epoch [3/4] | Batch [12/25] | Batch Loss: 0.1096\n",
      "Epoch [3/4] | Batch [13/25] | Batch Loss: 0.1184\n",
      "Epoch [3/4] | Batch [14/25] | Batch Loss: 0.1216\n",
      "Epoch [3/4] | Batch [15/25] | Batch Loss: 0.1638\n",
      "Epoch [3/4] | Batch [16/25] | Batch Loss: 0.1941\n",
      "Epoch [3/4] | Batch [17/25] | Batch Loss: 0.1121\n",
      "Epoch [3/4] | Batch [18/25] | Batch Loss: 0.1341\n",
      "Epoch [3/4] | Batch [19/25] | Batch Loss: 0.0905\n",
      "Epoch [3/4] | Batch [20/25] | Batch Loss: 0.1701\n",
      "Epoch [3/4] | Batch [21/25] | Batch Loss: 0.0699\n",
      "Epoch [3/4] | Batch [22/25] | Batch Loss: 0.1616\n",
      "Epoch [3/4] | Batch [23/25] | Batch Loss: 0.1679\n",
      "Epoch [3/4] | Batch [24/25] | Batch Loss: 0.0854\n",
      "Epoch [3/4] | Batch [25/25] | Batch Loss: 0.1590\n",
      "Epoch [3/4] | Train Loss: 0.1359 | Val Loss: 0.0818\n",
      "Epoch [4/4] | Batch [1/25] | Batch Loss: 0.1004\n",
      "Epoch [4/4] | Batch [2/25] | Batch Loss: 0.1584\n",
      "Epoch [4/4] | Batch [3/25] | Batch Loss: 0.0963\n",
      "Epoch [4/4] | Batch [4/25] | Batch Loss: 0.1780\n",
      "Epoch [4/4] | Batch [5/25] | Batch Loss: 0.0984\n",
      "Epoch [4/4] | Batch [6/25] | Batch Loss: 0.1705\n",
      "Epoch [4/4] | Batch [7/25] | Batch Loss: 0.1442\n",
      "Epoch [4/4] | Batch [8/25] | Batch Loss: 0.1216\n",
      "Epoch [4/4] | Batch [9/25] | Batch Loss: 0.1120\n",
      "Epoch [4/4] | Batch [10/25] | Batch Loss: 0.1066\n",
      "Epoch [4/4] | Batch [11/25] | Batch Loss: 0.0832\n",
      "Epoch [4/4] | Batch [12/25] | Batch Loss: 0.1293\n",
      "Epoch [4/4] | Batch [13/25] | Batch Loss: 0.0823\n",
      "Epoch [4/4] | Batch [14/25] | Batch Loss: 0.0782\n",
      "Epoch [4/4] | Batch [15/25] | Batch Loss: 0.0852\n",
      "Epoch [4/4] | Batch [16/25] | Batch Loss: 0.1076\n",
      "Epoch [4/4] | Batch [17/25] | Batch Loss: 0.0776\n",
      "Epoch [4/4] | Batch [18/25] | Batch Loss: 0.0952\n",
      "Epoch [4/4] | Batch [19/25] | Batch Loss: 0.1759\n",
      "Epoch [4/4] | Batch [20/25] | Batch Loss: 0.0967\n",
      "Epoch [4/4] | Batch [21/25] | Batch Loss: 0.0985\n",
      "Epoch [4/4] | Batch [22/25] | Batch Loss: 0.0758\n",
      "Epoch [4/4] | Batch [23/25] | Batch Loss: 0.1221\n",
      "Epoch [4/4] | Batch [24/25] | Batch Loss: 0.0950\n",
      "Epoch [4/4] | Batch [25/25] | Batch Loss: 0.0861\n",
      "Epoch [4/4] | Train Loss: 0.1110 | Val Loss: 0.0800\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a simple test on picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = \"a01-077.png\"\n",
    "# Open the image\n",
    "image = Image.open(example_input)\n",
    "transform=ParagraphStem(augment=False)\n",
    "image_tensor = transform(image)\n",
    "\n",
    "image_tensor=image_tensor.unsqueeze(0)\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    output = model(image_tensor)  # Encode image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S>And, since this is election gear in West\n",
      "Germany, Dr. Adenauer is in a tough\n",
      "spot. Joyce Egginton cables: President\n",
      "Kennedy at his Washington Press con-\n",
      "ference admitted he did not know\n",
      "Whether America was lagging behind\n",
      "Russia in missile power. He said he\n",
      "Was waiting for his senior military\n",
      "aides to come up with the answer on\n",
      "February 20.<E><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P>\n"
     ]
    }
   ],
   "source": [
    "decoded_text = ''.join(mapping[idx] for idx in output[0].tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LORA fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "target_module_1 = [\n",
    "    \"self_attn.in_proj_weight\",\n",
    "    \"self_attn.out_proj\",\n",
    "    \"multihead_attn.in_proj_weight\",\n",
    "    \"multihead_attn.out_proj\",\n",
    "    \"linear1\",\n",
    "    \"linear2\",\n",
    "]\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,   # Rank of decomposition\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_module_1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 65,536 || all params: 14,054,292 || trainable%: 0.4663\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Check trainable parameters\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=data_config[\"inverse_mapping\"][\"<P>\"])  # Ignore padding token\n",
    "optimizer = optim.AdamW(lora_model.parameters(), lr=0.0001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6] | Batch [1/25] | Batch Loss: 0.7589\n",
      "Epoch [1/6] | Batch [2/25] | Batch Loss: 0.6345\n",
      "Epoch [1/6] | Batch [3/25] | Batch Loss: 0.8879\n",
      "Epoch [1/6] | Batch [4/25] | Batch Loss: 0.8949\n",
      "Epoch [1/6] | Batch [5/25] | Batch Loss: 1.8075\n",
      "Epoch [1/6] | Batch [6/25] | Batch Loss: 1.1619\n",
      "Epoch [1/6] | Batch [7/25] | Batch Loss: 0.9878\n",
      "Epoch [1/6] | Batch [8/25] | Batch Loss: 0.6346\n",
      "Epoch [1/6] | Batch [9/25] | Batch Loss: 0.9811\n",
      "Epoch [1/6] | Batch [10/25] | Batch Loss: 1.3444\n",
      "Epoch [1/6] | Batch [11/25] | Batch Loss: 0.9411\n",
      "Epoch [1/6] | Batch [12/25] | Batch Loss: 0.6562\n",
      "Epoch [1/6] | Batch [13/25] | Batch Loss: 0.9765\n",
      "Epoch [1/6] | Batch [14/25] | Batch Loss: 0.6465\n",
      "Epoch [1/6] | Batch [15/25] | Batch Loss: 0.7510\n",
      "Epoch [1/6] | Batch [16/25] | Batch Loss: 1.0983\n",
      "Epoch [1/6] | Batch [17/25] | Batch Loss: 0.8488\n",
      "Epoch [1/6] | Batch [18/25] | Batch Loss: 0.5181\n",
      "Epoch [1/6] | Batch [19/25] | Batch Loss: 0.7783\n",
      "Epoch [1/6] | Batch [20/25] | Batch Loss: 0.9842\n",
      "Epoch [1/6] | Batch [21/25] | Batch Loss: 0.5945\n",
      "Epoch [1/6] | Batch [22/25] | Batch Loss: 0.7007\n",
      "Epoch [1/6] | Batch [23/25] | Batch Loss: 0.9534\n",
      "Epoch [1/6] | Batch [24/25] | Batch Loss: 1.0488\n",
      "Epoch [1/6] | Batch [25/25] | Batch Loss: 0.6834\n",
      "Epoch [1/6] | Train Loss: 0.8909 | Val Loss: 0.4730\n",
      "Epoch [2/6] | Batch [1/25] | Batch Loss: 1.8324\n",
      "Epoch [2/6] | Batch [2/25] | Batch Loss: 1.1219\n",
      "Epoch [2/6] | Batch [3/25] | Batch Loss: 0.9124\n",
      "Epoch [2/6] | Batch [4/25] | Batch Loss: 0.6915\n",
      "Epoch [2/6] | Batch [5/25] | Batch Loss: 0.7294\n",
      "Epoch [2/6] | Batch [6/25] | Batch Loss: 0.4727\n",
      "Epoch [2/6] | Batch [7/25] | Batch Loss: 0.9344\n",
      "Epoch [2/6] | Batch [8/25] | Batch Loss: 0.4414\n",
      "Epoch [2/6] | Batch [9/25] | Batch Loss: 0.8452\n",
      "Epoch [2/6] | Batch [10/25] | Batch Loss: 0.8022\n",
      "Epoch [2/6] | Batch [11/25] | Batch Loss: 0.7946\n",
      "Epoch [2/6] | Batch [12/25] | Batch Loss: 0.5390\n",
      "Epoch [2/6] | Batch [13/25] | Batch Loss: 0.7914\n",
      "Epoch [2/6] | Batch [14/25] | Batch Loss: 0.8089\n",
      "Epoch [2/6] | Batch [15/25] | Batch Loss: 0.7068\n",
      "Epoch [2/6] | Batch [16/25] | Batch Loss: 0.7583\n",
      "Epoch [2/6] | Batch [17/25] | Batch Loss: 1.0615\n",
      "Epoch [2/6] | Batch [18/25] | Batch Loss: 0.7006\n",
      "Epoch [2/6] | Batch [19/25] | Batch Loss: 0.8372\n",
      "Epoch [2/6] | Batch [20/25] | Batch Loss: 0.6735\n",
      "Epoch [2/6] | Batch [21/25] | Batch Loss: 1.2455\n",
      "Epoch [2/6] | Batch [22/25] | Batch Loss: 0.6451\n",
      "Epoch [2/6] | Batch [23/25] | Batch Loss: 1.2544\n",
      "Epoch [2/6] | Batch [24/25] | Batch Loss: 0.8958\n",
      "Epoch [2/6] | Batch [25/25] | Batch Loss: 0.5652\n",
      "Epoch [2/6] | Train Loss: 0.8425 | Val Loss: 0.4849\n",
      "Epoch [3/6] | Batch [1/25] | Batch Loss: 0.5969\n",
      "Epoch [3/6] | Batch [2/25] | Batch Loss: 0.8482\n",
      "Epoch [3/6] | Batch [3/25] | Batch Loss: 0.5941\n",
      "Epoch [3/6] | Batch [4/25] | Batch Loss: 0.4535\n",
      "Epoch [3/6] | Batch [5/25] | Batch Loss: 0.7316\n",
      "Epoch [3/6] | Batch [6/25] | Batch Loss: 0.6498\n",
      "Epoch [3/6] | Batch [7/25] | Batch Loss: 0.6793\n",
      "Epoch [3/6] | Batch [8/25] | Batch Loss: 0.9263\n",
      "Epoch [3/6] | Batch [9/25] | Batch Loss: 1.1221\n",
      "Epoch [3/6] | Batch [10/25] | Batch Loss: 1.0655\n",
      "Epoch [3/6] | Batch [11/25] | Batch Loss: 0.6526\n",
      "Epoch [3/6] | Batch [12/25] | Batch Loss: 0.7920\n",
      "Epoch [3/6] | Batch [13/25] | Batch Loss: 0.6794\n",
      "Epoch [3/6] | Batch [14/25] | Batch Loss: 0.8563\n",
      "Epoch [3/6] | Batch [15/25] | Batch Loss: 0.8008\n",
      "Epoch [3/6] | Batch [16/25] | Batch Loss: 0.8440\n",
      "Epoch [3/6] | Batch [17/25] | Batch Loss: 0.8336\n",
      "Epoch [3/6] | Batch [18/25] | Batch Loss: 0.6749\n",
      "Epoch [3/6] | Batch [19/25] | Batch Loss: 0.8984\n",
      "Epoch [3/6] | Batch [20/25] | Batch Loss: 0.5018\n",
      "Epoch [3/6] | Batch [21/25] | Batch Loss: 0.6576\n",
      "Epoch [3/6] | Batch [22/25] | Batch Loss: 0.7692\n",
      "Epoch [3/6] | Batch [23/25] | Batch Loss: 0.4419\n",
      "Epoch [3/6] | Batch [24/25] | Batch Loss: 0.4716\n",
      "Epoch [3/6] | Batch [25/25] | Batch Loss: 0.6210\n",
      "Epoch [3/6] | Train Loss: 0.7265 | Val Loss: 0.3900\n",
      "Epoch [4/6] | Batch [1/25] | Batch Loss: 0.5887\n",
      "Epoch [4/6] | Batch [2/25] | Batch Loss: 0.9503\n",
      "Epoch [4/6] | Batch [3/25] | Batch Loss: 0.4779\n",
      "Epoch [4/6] | Batch [4/25] | Batch Loss: 0.8435\n",
      "Epoch [4/6] | Batch [5/25] | Batch Loss: 0.7057\n",
      "Epoch [4/6] | Batch [6/25] | Batch Loss: 0.5225\n",
      "Epoch [4/6] | Batch [7/25] | Batch Loss: 1.0366\n",
      "Epoch [4/6] | Batch [8/25] | Batch Loss: 0.5655\n",
      "Epoch [4/6] | Batch [9/25] | Batch Loss: 0.5672\n",
      "Epoch [4/6] | Batch [10/25] | Batch Loss: 0.8207\n",
      "Epoch [4/6] | Batch [11/25] | Batch Loss: 0.6499\n",
      "Epoch [4/6] | Batch [12/25] | Batch Loss: 0.7313\n",
      "Epoch [4/6] | Batch [13/25] | Batch Loss: 0.4234\n",
      "Epoch [4/6] | Batch [14/25] | Batch Loss: 0.6051\n",
      "Epoch [4/6] | Batch [15/25] | Batch Loss: 0.5660\n",
      "Epoch [4/6] | Batch [16/25] | Batch Loss: 0.4074\n",
      "Epoch [4/6] | Batch [17/25] | Batch Loss: 0.3443\n",
      "Epoch [4/6] | Batch [18/25] | Batch Loss: 0.9866\n",
      "Epoch [4/6] | Batch [19/25] | Batch Loss: 0.6784\n",
      "Epoch [4/6] | Batch [20/25] | Batch Loss: 0.7978\n",
      "Epoch [4/6] | Batch [21/25] | Batch Loss: 0.6300\n",
      "Epoch [4/6] | Batch [22/25] | Batch Loss: 0.3798\n",
      "Epoch [4/6] | Batch [23/25] | Batch Loss: 0.6425\n",
      "Epoch [4/6] | Batch [24/25] | Batch Loss: 0.5620\n",
      "Epoch [4/6] | Batch [25/25] | Batch Loss: 0.4868\n",
      "Epoch [4/6] | Train Loss: 0.6388 | Val Loss: 0.3297\n",
      "Epoch [5/6] | Batch [1/25] | Batch Loss: 0.4763\n",
      "Epoch [5/6] | Batch [2/25] | Batch Loss: 0.4986\n",
      "Epoch [5/6] | Batch [3/25] | Batch Loss: 0.7304\n",
      "Epoch [5/6] | Batch [4/25] | Batch Loss: 0.5303\n",
      "Epoch [5/6] | Batch [5/25] | Batch Loss: 0.5565\n",
      "Epoch [5/6] | Batch [6/25] | Batch Loss: 0.5176\n",
      "Epoch [5/6] | Batch [7/25] | Batch Loss: 0.3049\n",
      "Epoch [5/6] | Batch [8/25] | Batch Loss: 0.6390\n",
      "Epoch [5/6] | Batch [9/25] | Batch Loss: 0.5315\n",
      "Epoch [5/6] | Batch [10/25] | Batch Loss: 0.6664\n",
      "Epoch [5/6] | Batch [11/25] | Batch Loss: 0.5189\n",
      "Epoch [5/6] | Batch [12/25] | Batch Loss: 0.6433\n",
      "Epoch [5/6] | Batch [13/25] | Batch Loss: 0.8779\n",
      "Epoch [5/6] | Batch [14/25] | Batch Loss: 0.4292\n",
      "Epoch [5/6] | Batch [15/25] | Batch Loss: 0.4419\n",
      "Epoch [5/6] | Batch [16/25] | Batch Loss: 0.5426\n",
      "Epoch [5/6] | Batch [17/25] | Batch Loss: 0.5971\n",
      "Epoch [5/6] | Batch [18/25] | Batch Loss: 0.9696\n",
      "Epoch [5/6] | Batch [19/25] | Batch Loss: 0.7274\n",
      "Epoch [5/6] | Batch [20/25] | Batch Loss: 0.5130\n",
      "Epoch [5/6] | Batch [21/25] | Batch Loss: 0.7295\n",
      "Epoch [5/6] | Batch [22/25] | Batch Loss: 0.9077\n",
      "Epoch [5/6] | Batch [23/25] | Batch Loss: 0.2997\n",
      "Epoch [5/6] | Batch [24/25] | Batch Loss: 0.7328\n",
      "Epoch [5/6] | Batch [25/25] | Batch Loss: 0.5941\n",
      "Epoch [5/6] | Train Loss: 0.5990 | Val Loss: 0.3191\n",
      "Epoch [6/6] | Batch [1/25] | Batch Loss: 0.4573\n",
      "Epoch [6/6] | Batch [2/25] | Batch Loss: 0.3837\n",
      "Epoch [6/6] | Batch [3/25] | Batch Loss: 0.5968\n",
      "Epoch [6/6] | Batch [4/25] | Batch Loss: 0.3816\n",
      "Epoch [6/6] | Batch [5/25] | Batch Loss: 0.9744\n",
      "Epoch [6/6] | Batch [6/25] | Batch Loss: 0.5796\n",
      "Epoch [6/6] | Batch [7/25] | Batch Loss: 0.5734\n",
      "Epoch [6/6] | Batch [8/25] | Batch Loss: 0.4421\n",
      "Epoch [6/6] | Batch [9/25] | Batch Loss: 0.6482\n",
      "Epoch [6/6] | Batch [10/25] | Batch Loss: 0.4747\n",
      "Epoch [6/6] | Batch [11/25] | Batch Loss: 0.7878\n",
      "Epoch [6/6] | Batch [12/25] | Batch Loss: 0.3944\n",
      "Epoch [6/6] | Batch [13/25] | Batch Loss: 0.5801\n",
      "Epoch [6/6] | Batch [14/25] | Batch Loss: 0.4683\n",
      "Epoch [6/6] | Batch [15/25] | Batch Loss: 0.5137\n",
      "Epoch [6/6] | Batch [16/25] | Batch Loss: 0.6151\n",
      "Epoch [6/6] | Batch [17/25] | Batch Loss: 0.5846\n",
      "Epoch [6/6] | Batch [18/25] | Batch Loss: 0.9163\n",
      "Epoch [6/6] | Batch [19/25] | Batch Loss: 0.4691\n",
      "Epoch [6/6] | Batch [20/25] | Batch Loss: 1.1722\n",
      "Epoch [6/6] | Batch [21/25] | Batch Loss: 0.5679\n",
      "Epoch [6/6] | Batch [22/25] | Batch Loss: 0.6255\n",
      "Epoch [6/6] | Batch [23/25] | Batch Loss: 0.3859\n",
      "Epoch [6/6] | Batch [24/25] | Batch Loss: 0.5097\n",
      "Epoch [6/6] | Batch [25/25] | Batch Loss: 0.3645\n",
      "Epoch [6/6] | Train Loss: 0.5787 | Val Loss: 0.2830\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train_model(lora_model, train_loader, val_loader, optimizer, criterion, device, num_epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the single picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = \"a01-077.png\"\n",
    "# Open the image\n",
    "image = Image.open(example_input)\n",
    "transform=ParagraphStem(augment=False)\n",
    "image_tensor = transform(image)\n",
    "\n",
    "image_tensor=image_tensor.unsqueeze(0)\n",
    "\n",
    "lora_model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    output = lora_model(image_tensor)  # Encode image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S>And, since this is election year in West\n",
      "Germany, Dr. Adenauer is in a tough\n",
      "spot. Joyce Egginton cables: President\n",
      "ference admitted he did not know\n",
      "ference admitted he did not know\n",
      "Russia in missile power. He said he\n",
      "Russia in missile power. He said he\n",
      "was waiting for his senior military\n",
      "aides to come Up with the answer on\n",
      "February 2.<E><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P><P>\n"
     ]
    }
   ],
   "source": [
    "decoded_text = ''.join(mapping[idx] for idx in output[0].tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
